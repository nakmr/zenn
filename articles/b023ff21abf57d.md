---
title: "LLMを利用したアプリ開発者のための責任あるAI利用ガイドライン"
emoji: "🐡"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: []
published: false
---

<!-- textlint-disable -->
:::details change log
2024/XX/XX 初版公開
:::
<!-- textlint-enable -->

<!-- textlint-disable -->
:::message
この文章には生成AIの出力が含まれています。
:::
<!-- textlint-enable -->

# 責任あるAI利用ガイドライン
*LLMを外部API経由で利用するアプリケーション開発者向け*

# 1. はじめに

大規模言語モデル（LLM）のAPIを活用することで、アプリケーションに高度な言語処理機能を組み込むことが可能となります。
その開発過程では、さまざまな創造的なアイデアを試したり、新しい技術を学んだりする楽しさがあります。
一方で、特にベンダが提供する外部APIを利用する場合には、プロジェクトによって多くの制約が伴うことも事実です。

特に、私たちのようにお客さまの情報を取り扱ったり、お客さまの環境で作業を行う場合には、情報の取り扱いに対して特別な配慮が求められます。
しかし、「気をつけろ！」と言われても、具体的に何を気をつければよいのか、どのような背景があるのか、そして何を「すべき」または「すべきでない」のかが曖昧な場合が少なくありません。

そこで本ガイドラインでは、LLM利用に伴うリスクの背景を整理し、それに対応する具体的な実践方法を提供します。
これらの内容は、ベンダ各社が公開している情報や、関連する記事や書籍をもとに構築しています。
責任あるAI利用の一助として、このガイドラインをご活用いただければ幸いです。

---

## 2. 基本原則
Microsoftの責任あるAIの6つの原則に基づき、以下の点を重視します：

- **公平性**: すべてのユーザーに対して公平に機能し、特定のグループに不利益を与えないようにする。
- **信頼性と安全性**: 一貫して安全に動作し、予期しない動作を防ぐ。
- **プライバシーとセキュリティ**: ユーザーのデータを適切に保護し、無断で使用しない。
- **包括性**: 多様なユーザーのニーズに対応し、誰もが利用できるようにする。
- **透明性**: AIの動作や意思決定プロセスを明確にし、ユーザーが理解できるようにする。
- **説明責任**: AIの利用に伴う結果に対して責任を持ち、必要に応じて対応する。

---

## 3. 主なリスクと対策
### 3.1 データプライバシーの侵害
- **リスク**: ユーザーの個人情報や機密データが外部に漏洩する可能性。
- **対策**:
  - 機密情報をAPIに送信しない。
  - データを送信前に匿名化またはマスキングする。
  - API提供者のデータ保存ポリシーを確認し、必要に応じてユーザーの同意を得る。

### 3.2 偏見や差別
- **リスク**: LLMが訓練データのバイアスを引き継ぎ、不公平な結果を生成する可能性。
- **対策**:
  - 出力結果を定期的にレビューし、偏見が含まれていないか確認する。
  - 必要に応じてフィルタリングや修正を行う。

### 3.3 説明可能性の欠如
- **リスク**: ユーザーがAIの意思決定プロセスを理解できず、不信感を抱く可能性。
- **対策**:
  - AIの出力結果に対して、理由や根拠を説明できるようにする。
  - ユーザーに対して、AIの限界や不確実性を明示する。

### 3.4 信頼性と安全性の問題
- **リスク**: AIが誤った情報や有害なコンテンツを生成する可能性。
- **対策**:
  - 高リスクな分野では、AIの出力を人間が確認・監督するプロセスを導入する。
  - 不適切な出力を検出・防止するフィルタリング機能を実装する。

### 3.5 説明責任の欠如
- **リスク**: AIの誤った出力によりユーザーが損害を被った場合、責任の所在が不明確になる可能性。
- **対策**:
  - AIの出力に対して最終的な責任を持つ人間の監督者を設ける。
  - ユーザーに対して、AIの出力が参考情報であり、最終判断はユーザー自身が行うべきことを明示する。

---

## 4. 実践的なガイドライン
### 4.1 データ入力時の注意点
- 個人情報や機密情報をAPIに送信しない。
- 送信前にデータを匿名化またはマスキングする。

### 4.2 出力結果の検証
- AIの出力を人間が確認し、必要に応じて修正する。
- 特に高リスクな分野では、複数人でのレビュー体制を整える。

### 4.3 ユーザーへの情報提供
- AIが生成したコンテンツであることを明示する。
- AIの限界や不確実性についてユーザーに説明する。

### 4.4 継続的なモニタリングと改善
- AIの出力結果を定期的にレビューし、問題があれば修正する。
- ユーザーからのフィードバックを収集し、システムの改善に活用する。

---

## 5. チェックリスト
- [ ] 機密情報や個人情報をAPIに送信していないか？
- [ ] AIの出力結果に偏見や差別が含まれていないか？
- [ ] ユーザーに対してAIの利用に関する情報を適切に提供 
- [ ] - [ ] ユーザーに対してAIの利用に関する情報を適切に提供しているか？
- [ ] 不適切な出力を検出し、対応する仕組みが実装されているか？
- [ ] APIのプライバシーポリシーや利用規約を十分に理解し、遵守しているか？
- [ ] サービス停止やコスト増加などのリスクを考慮し、代替策を用意しているか？

---

## 6. FAQ
### Q1. LLMのAPIを利用しても安全ですか？
A1. 適切なデータ管理と出力の検証を行うことで、安全に利用できます。特に、機密情報を入力せず、出力内容を人間が確認することが重要です。

### Q2. LLMの出力が不適切だった場合、どう対処すればよいですか？
A2. 不適切な出力は破棄し、システムのログに記録して原因を特定します。また、APIプロバイダーにフィードバックを送信することで改善を促すことができます。

### Q3. APIの利用コストを管理する方法は？
A3. トークン使用量をモニタリングし、必要に応じて低コストなモデルや最適化されたリクエスト設定を検討してください。

---

## 7. 推奨リソース
- **ベンダのドキュメント**:
  - Microsoft
    - [責任ある AI とは?](https://learn.microsoft.com/ja-jp/azure/machine-learning/concept-responsible-ai?view=azureml-api-2)
- **記事**:
  - [LLMにとってやばい要素リスト](https://zenn.dev/acntechjp/articles/794fe6506c41c2)

---

## 8. まとめ
LLMを活用することで、アプリケーションに高い付加価値を提供できますが、それに伴うリスクや課題も慎重に管理する必要があります。本ガイドラインを参考に、責任あるAI利用を実践し、ユーザーにとって安全かつ信頼性の高いシステムを構築してください。

---

（本ガイドラインは初期バージョンです。フィードバックや追加のご提案があれば、ぜひお知らせください。）